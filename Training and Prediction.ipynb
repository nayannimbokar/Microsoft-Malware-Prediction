{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model training and prediction","metadata":{}},{"cell_type":"markdown","source":"This file implements the LightGBM method. Hyperparameters are set and the model is trained. Afterwards the model is saved in `../models/model_M1.p`.\n\nValidating the model with values from the test set is implemented at the end. A list of all used features sorted by their importance is also printed out.","metadata":{}},{"cell_type":"markdown","source":"***\n\nOptional befor training the model (not mentioned in this code):\n\nTo gain a good fitting model, hyperparameters are tuned. Bayesian Hyperparameter Optimization was used for that job (with the hyperopt-library).\n\nIn this process, a 3 folded Cross Validation was used to return the best fitted model.\n\n***","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## import libraries / delcare functions","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport lightgbm as lgb\nfrom tqdm import tqdm\nimport pickle\nimport time\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, precision_score, recall_score\nfrom sklearn.metrics import plot_confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score","metadata":{"ExecuteTime":{"end_time":"2019-03-20T06:58:03.029133Z","start_time":"2019-03-20T06:58:00.562385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = time.time() \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type == object:\n            df[col] = df[col].astype('category')\n        else: \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n            \n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"ExecuteTime":{"end_time":"2019-03-20T06:58:03.079994Z","start_time":"2019-03-20T06:58:03.068833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Data","metadata":{}},{"cell_type":"code","source":"%%time\nfiletrain = '../input/final-all-data-cleaning/finall_train_clean.csv'\nddf = dd.read_csv(filetrain)\ntrain = ddf.compute()\n#train=train.head(100)\nprint(\"train\")\ntrain = reduce_mem_usage(train)","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:00:26.507813Z","start_time":"2019-03-20T06:58:03.1069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train=pd.read_csv('../input/final-all-data-cleaning/finall_train_clean.csv',nrows=1000)\ntrain=train.sample(n=100, random_state=99)\nprint(\"train\")\ntrain = reduce_mem_usage(train)","metadata":{}},{"cell_type":"code","source":"%%time\nfiletest = '../input/final-all-data-cleaning/finall_test_clean.csv'\nddf = dd.read_csv(filetest)\ntest = ddf.compute()\n#test=test.head(100)\nprint(\"test\")\ntest = reduce_mem_usage(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test=pd.read_csv('../input/final-all-data-cleaning/finall_test_clean.csv',nrows=1000)\ntest=test.sample(n=100, random_state=99)\nprint(\"test\")\ntest = reduce_mem_usage(test)","metadata":{}},{"cell_type":"code","source":"# target column\ntarget = 'HasDetections'\n# id from data set\ndata_id = 'MachineIdentifier'","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:00:34.364698Z","start_time":"2019-03-20T07:00:34.355721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train.info()","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:00:42.100363Z","start_time":"2019-03-20T07:00:42.092354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test.info()","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:00:50.160709Z","start_time":"2019-03-20T07:00:50.156721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build up a list with all the features, which should be encoded via frequency\nlist_frequency_encoding = ['AppVersion',\n 'Census_OSVersion',\n 'EngineVersion']\n#list_frequency_encoding.append('AvSigVersion')\n#list_frequency_encoding.append('OsBuildLab')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build up a list with all the features, which should be encoded by label (part 1)\ncorrect_feature_by_hand = ['AppVersion',\n 'Census_ActivationChannel',\n 'Census_ChassisTypeName',\n 'Census_DeviceFamily',\n 'Census_FlightRing',\n 'Census_GenuineStateName',\n 'Census_MDC2FormFactor',\n 'Census_OSArchitecture',\n 'Census_OSBranch',\n 'Census_OSEdition',\n 'Census_OSInstallTypeName',\n 'Census_OSSkuName',\n 'Census_OSVersion',\n 'Census_OSWUAutoUpdateOptionsName',\n 'Census_PowerPlatformRoleName',\n 'Census_PrimaryDiskTypeName',\n 'EngineVersion',\n 'MachineIdentifier',\n 'OsPlatformSubRelease',\n 'OsVer',\n 'Platform',\n 'Processor',\n 'SkuEdition',\n 'SmartScreen',\n 'Census_MDC2FormFactor_new',\n 'ProductName',\n 'SmartScreen_AVProductsInstalled',\n 'primary_drive_c_ratio',\n 'non_primary_drive_MB',\n  'aspect_ratio',\n  'monitor_dims',\n  'Screen_Area',\n  'ram_per_processor',\n  'ProcessorCoreCount_DisplaySizeInInches',\n  'AVProductsInstalled']\n\n#correct_feature_by_hand.append('AvSigVersion')\n#correct_feature_by_hand.append('OsBuildLab')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build up a list with all the features, which should be encoded by label (part 2)\nlist_label_encoding = list(set(correct_feature_by_hand)-set(list_frequency_encoding))\nlist_label_encoding.remove(data_id) # We don't want to encode this feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function for frequency encoding\ndef frequency_encoding(feature):\n    # Count the number of values of each feature and reset the indices\n    t = pd.concat([train[feature], test[feature]]).value_counts().reset_index()\n    # Building up a new index (old index is set by default to 'level_0')\n    t = t.reset_index()\n    # Set the old index 'level_0' for all values, which only occur once, to NaN\n    t.loc[t[feature] == 1, 'level_0'] = np.nan\n    # Reset the original index (= the value name) as index\n    t.set_index('index', inplace=True)\n    # return the number of values , which occur two or more times, +1\n    max_label = t['level_0'].max() + 1\n    # fill all nan-values to max_label\n    t.fillna(max_label, inplace=True)\n    return t.to_dict()['level_0']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in tqdm(list_frequency_encoding):\n    freq_enc_dict = frequency_encoding(col)\n    train[col] = train[col].map(lambda x: freq_enc_dict.get(x, np.nan)).astype('float32')\n    test[col] = test[col].map(lambda x: freq_enc_dict.get(x, np.nan)).astype('float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in tqdm(list_label_encoding):\n    freq_enc_dict = frequency_encoding(col)\n    train[col] = train[col].map(lambda x: freq_enc_dict.get(x, np.nan)).astype('float32')\n    test[col] = test[col].map(lambda x: freq_enc_dict.get(x, np.nan)).astype('float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"## prepare files for training and prediction","metadata":{}},{"cell_type":"code","source":"# labels from train file\ntrain_labels = np.array(train['HasDetections'].astype(np.int8)).reshape((-1, ))","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:02:18.073531Z","start_time":"2019-03-20T07:02:18.000744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove target and data_id for model training\ntrain = train.drop(columns = [target, data_id])","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:02:27.273101Z","start_time":"2019-03-20T07:02:25.635767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save data_id for submission file\ntest_ids = list(test[data_id])","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:02:44.190555Z","start_time":"2019-03-20T07:02:43.506277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove target and data_id for prediction\ntest = test.drop(columns = [target, data_id])\n#test = test.drop(test.columns[[0]], axis=1)","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:02:58.878686Z","start_time":"2019-03-20T07:02:57.479961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(train,train_labels, random_state = 24)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set parameters for LightGBM algorithm","metadata":{}},{"cell_type":"markdown","source":"# Categorical features for LighGBM parameter\ncategorical_feature = ['OsPlatformSubRelease',\n 'Census_MDC2FormFactor_new',\n 'Census_FlightRing',\n 'Census_PrimaryDiskTypeName',\n 'Census_OSSkuName',\n 'Census_OSBranch',\n 'OsVer',\n 'SkuEdition',\n 'Census_OSArchitecture',\n 'Census_OSEdition',\n 'Census_GenuineStateName',\n 'Processor',\n 'SmartScreen',\n 'Census_OSInstallTypeName',\n 'Census_OSWUAutoUpdateOptionsName',\n 'Census_ChassisTypeName',\n 'Census_MDC2FormFactor',\n 'Platform',\n 'Census_DeviceFamily',\n 'Census_ActivationChannel',\n 'Census_PowerPlatformRoleName',\n 'Census_ProcessorCoreCount',\n 'Census_HasOpticalDiskDrive',\n 'Census_IsAlwaysOnAlwaysConnectedCapable',\n 'Census_IsPenCapable',\n 'Census_IsPortableOperatingSystem',\n 'Census_IsSecureBootEnabled',\n 'Census_IsTouchEnabled',\n 'Census_IsVirtualDevice',\n 'Firewall',\n 'HasTpm',\n 'IsProtected',\n 'IsSxsPassiveMode',\n 'SMode',\n 'Wdft_IsGamer']","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:03:19.878833Z","start_time":"2019-03-20T07:03:19.873876Z"}}},{"cell_type":"markdown","source":"categorical_feature = ['AppVersion',\n 'Census_ActivationChannel',\n 'Census_ChassisTypeName',\n 'Census_DeviceFamily',\n 'Census_FlightRing',\n 'Census_GenuineStateName',\n 'Census_MDC2FormFactor',\n 'Census_OSArchitecture',\n 'Census_OSBranch',\n 'Census_OSEdition',\n 'Census_OSInstallTypeName',\n 'Census_OSSkuName',\n 'Census_OSVersion',\n 'Census_OSWUAutoUpdateOptionsName',\n 'Census_PowerPlatformRoleName',\n 'Census_PrimaryDiskTypeName',\n 'EngineVersion',\n 'OsPlatformSubRelease',\n 'OsVer',\n 'Platform',\n 'Processor',\n 'SkuEdition',\n 'SmartScreen',\n 'Census_MDC2FormFactor_new',\n 'ProductName',\n 'SmartScreen_AVProductsInstalled',\n 'primary_drive_c_ratio',\n 'non_primary_drive_MB',\n  'aspect_ratio',\n  'monitor_dims',\n  'Screen_Area',\n  'ram_per_processor',\n  'ProcessorCoreCount_DisplaySizeInInches',\n  'AVProductsInstalled']","metadata":{}},{"cell_type":"code","source":"best_hyp = {'boosting_type': 'gbdt',\n 'class_weight': None,\n 'colsample_bytree': 0.6110437067662637,\n 'learning_rate': 0.0106,\n 'min_child_samples': 295,\n 'num_leaves': 160,\n 'reg_alpha': 0.6321152748961743,\n 'reg_lambda': 0.6313659622714517,\n 'subsample_for_bin': 80000,\n 'subsample': 0.8202307264855064}","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:03:30.780828Z","start_time":"2019-03-20T07:03:30.776838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimators = 12000","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:03:41.520525Z","start_time":"2019-03-20T07:03:41.517532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"markdown","source":"# fast & efficient way to calculate mean using numba\n@jit(parallel=True)\ndef mean_score(array):\n    return np.mean(array)\n\n# fast & efficient way to calculate division using numba\n@jit(parallel=True)\ndef jit_div(array, divisor):\n    return np.divide(array, divisor)\n\ndef run_lgb(train, target, test):\n    global clf\n    \n    kf = KFold(n_splits=3, shuffle=True)\n\n    train_scores = []\n    cv_scores = []\n    train_predictions = np.zeros(train.shape[0], dtype='float32')\n    test_predictions = np.zeros(test.shape[0], dtype='float32')\n    \n    for (train_index, test_index) in tqdm(kf.split(train, target)):\n        x_train, y_train = train.iloc[train_index], target.iloc[train_index]\n        x_test, y_test = train.iloc[test_index], target.iloc[test_index]\n\n        clf = lgb.LGBMClassifier(n_estimators=estimators, n_jobs = -1, \n                                       objective = 'binary', random_state = 50, **best_hyp)\n        \n        clf.fit(x_train, y_train, eval_metric='auc',  \n                eval_set=[(x_test, y_test)], \n                verbose=200,\n                early_stopping_rounds=100)\n        pred = clf.predict(x_train)\n        score = roc_auc_score(y_train, pred)\n        train_scores.append(score)\n\n        pred = clf.predict(x_test)\n        score = roc_auc_score(y_test, pred)\n        cv_scores.append(score)\n        \n        # predict probabilities for train\n        pred_prob = clf.predict_proba(train)[:,1]\n        train_predictions = np.add(train_predictions, pred_prob).astype('float32')\n        \n        # predict probabilities for test\n        pred_prob = clf.predict_proba(test)[:,1]\n        test_predictions = np.add(test_predictions, pred_prob).astype('float32')\n    \n    print(f'training sklearn auc: {mean_score(train_scores):.4f}')\n    print(f'3-fold CV sklearn auc: {mean_score(cv_scores):.4f}')\n\n    return jit_div(train_predictions, 3), jit_div(test_predictions, 3)","metadata":{}},{"cell_type":"markdown","source":"%%time\ntrain_predictions, test_predictions = run_lgb(train, target, test)","metadata":{}},{"cell_type":"markdown","source":"#display feature importances\nfeat_imp = pd.Series(clf.feature_importances_, index=train.columns)\nfeat_imp.nlargest(30).plot(kind='barh', figsize=(10,10))","metadata":{}},{"cell_type":"markdown","source":"# plot the kernel densities of predicted train probabilities\nsns.kdeplot(train_predictions)\nplt.show()","metadata":{}},{"cell_type":"markdown","source":"# plot the kernel densities of predicted test probabilities\nsns.kdeplot(test_predictions)\nplt.show()","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Re-create the best model and train on full training data\nmodel = lgb.LGBMClassifier(n_estimators=estimators, n_jobs = -1, \n                                       objective = 'binary', random_state = 50, **best_hyp)","metadata":{"ExecuteTime":{"end_time":"2019-03-20T07:04:51.61542Z","start_time":"2019-03-20T07:04:51.612447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#model.fit(X_train, y_train)\n","metadata":{"ExecuteTime":{"end_time":"2019-03-20T08:57:41.658138Z","start_time":"2019-03-20T07:05:02.511896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## save model","metadata":{}},{"cell_type":"code","source":"%%time\nimport pickle\n\n#pickle.dump(model, open(\"./lightm_model_f10000.p\", \"wb\"))\nmodel = pickle.load(open(\"../input/final-lightbgm-code/lightm_model_f10000.p\", \"rb\"))","metadata":{"ExecuteTime":{"end_time":"2019-03-20T08:59:39.599997Z","start_time":"2019-03-20T08:59:36.197405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross Validation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (7,7))\nplot_confusion_matrix(model,X_test, y_test,values_format='.0f')\ny_pred = model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nauc_score = roc_auc_score(y_test, y_pred)\nprint(f'Validation sklearn mean auc score: {auc_score:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"false_positive_rate, true_positive_rate, threshold = roc_curve(y_test,y_pred)\nplt.subplots(1, figsize=(8,8))\nplt.title('Receiver Operating Characteristic - Light GBM')\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# predict on Test set","metadata":{}},{"cell_type":"markdown","source":"%%time\npreds = model.predict_proba(test)[:, 1]","metadata":{"ExecuteTime":{"end_time":"2019-03-20T09:40:32.897513Z","start_time":"2019-03-20T08:59:50.507097Z"}}},{"cell_type":"markdown","source":"%%time\nimport pandas as pd\n# create submission\n#submission = pd.DataFrame({data_id: test_ids, target: preds})\nsubmission=pd.read_csv('../input/lgbtm-model11/Submission_M1.csv')","metadata":{"ExecuteTime":{"end_time":"2019-03-20T09:42:00.60551Z","start_time":"2019-03-20T09:42:00.017688Z"}}},{"cell_type":"markdown","source":"submission.head()","metadata":{"ExecuteTime":{"end_time":"2019-03-20T09:43:40.979652Z","start_time":"2019-03-20T09:43:40.956679Z"}}},{"cell_type":"markdown","source":"preds=submission['HasDetections']","metadata":{}},{"cell_type":"markdown","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.kdeplot(preds)\nplt.xlabel('test predictions') \nplt.ylabel('Probability Density')","metadata":{}},{"cell_type":"markdown","source":"plt.figure(figsize=(15,7))\nplt.hist(preds[y_train==0], bins=50, label='Negatives')\nplt.hist(preds[y_train==1], bins=50, label='Positives', alpha=0.7, color='r')\nplt.xlabel('Probability of being Positive Class', fontsize=25)\nplt.ylabel('Number of records in each bucket', fontsize=25)\nplt.legend(fontsize=15)\nplt.tick_params(axis='both', labelsize=25, pad=5)\nplt.show() ","metadata":{}},{"cell_type":"markdown","source":"mu, sigma = 200, 25\nn, bins, patches = plt.hist(preds)\nplt.xlabel('Probability') \nplt.ylabel('Count')\nplt.show()\n","metadata":{}},{"cell_type":"code","source":"%%time\n# save submission file\n#submission.to_csv('./Submission_f1111.csv', index = False)","metadata":{"ExecuteTime":{"end_time":"2019-03-20T09:44:16.585877Z","start_time":"2019-03-20T09:43:54.415399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance","metadata":{}},{"cell_type":"code","source":"# list of all features\nfeature_names = list(train.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature_importance(importance,names,model_type):\n\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(20,16))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances=model.feature_importances_\nplot_feature_importance(feature_importances,train.columns,'Light GBM')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create feature importance\nfeature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort for most important\nfeature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\npd.options.display.max_rows\npd.set_option('display.max_rows', None)\nprint(feature_importances)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate normalized and cumulative feature importance\n#feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n#feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature_importances.to_csv('./FeatureImportance_M1.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert(seconds):\n    return time.strftime(\"time for notebook is: %H hrs:%M min:%S seconds\", time.gmtime(n))\n      \nn =time.time()-t\nprint(convert(n))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}